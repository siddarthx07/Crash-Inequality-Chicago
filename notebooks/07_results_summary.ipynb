{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Summary for Paper\n",
    "\n",
    "This notebook consolidates all analysis results into publication-ready tables and summaries for the final report.\n",
    "\n",
    "**Outputs:**\n",
    "1. Model performance comparison table\n",
    "2. Inequality analysis summary table\n",
    "3. Spatial analysis summary table\n",
    "4. Key findings summary\n",
    "5. LaTeX-formatted tables for ACM SIG paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.max_columns', 120)\n",
    "pd.set_option('display.width', 160)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "# Paths\n",
    "RESULTS_DIR = Path('../results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Globals populated as files are loaded\n",
    "HAS_MODEL_RESULTS = False\n",
    "model_summary = None\n",
    "temporal_test = None\n",
    "top_features = None\n",
    "inequality_table = None\n",
    "stat_tests = None\n",
    "moran_table = None\n",
    "lisa_summary = None\n",
    "dataset_stats = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded temporal model results\n",
      "\n",
      "====================================================================================================\n",
      "TABLE 1: MODEL PERFORMANCE (Temporal Validation)\n",
      "====================================================================================================\n",
      "              Split Type            Model  PR-AUC  ROC-AUC  F1@0.5  Best F1  Threshold\n",
      "Temporal (Chronological) GradientBoosting   0.769    0.947   0.667    0.686      0.366\n",
      "Temporal (Chronological)         Logistic   0.762    0.945   0.591    0.684      0.806\n",
      "Temporal (Chronological)     RandomForest   0.754    0.942   0.653    0.675      0.340\n",
      "\n",
      "\n",
      "✅ Saved: table1_model_performance.csv and .tex\n"
     ]
    }
   ],
   "source": [
    "# Load model performance results (temporal split)\n",
    "MODEL_COMPARISON_PATH = RESULTS_DIR / 'temporal_model_comparison.csv'\n",
    "MODEL_TEST_PATH = RESULTS_DIR / 'temporal_test_metrics.csv'\n",
    "\n",
    "if MODEL_COMPARISON_PATH.exists() and MODEL_TEST_PATH.exists():\n",
    "    model_summary = pd.read_csv(MODEL_COMPARISON_PATH)\n",
    "    temporal_test = pd.read_csv(MODEL_TEST_PATH)\n",
    "    HAS_MODEL_RESULTS = True\n",
    "\n",
    "    # Clean/rename columns for presentation\n",
    "    rename_cols = {\n",
    "        'model': 'Model',\n",
    "        'pr_auc': 'PR-AUC',\n",
    "        'roc_auc': 'ROC-AUC',\n",
    "        'f1@0.5': 'F1@0.5',\n",
    "        'best_f1': 'Best F1',\n",
    "        'best_thresh': 'Threshold'\n",
    "    }\n",
    "    model_summary = model_summary.rename(columns={k: v for k, v in rename_cols.items() if k in model_summary.columns})\n",
    "\n",
    "    if 'Split Type' not in model_summary.columns:\n",
    "        model_summary.insert(0, 'Split Type', 'Temporal (Chronological)')\n",
    "\n",
    "    display_cols = [\n",
    "        col for col in ['Split Type', 'Model', 'PR-AUC', 'ROC-AUC', 'F1@0.5', 'Best F1', 'Threshold']\n",
    "        if col in model_summary.columns\n",
    "    ]\n",
    "    display_table = model_summary[display_cols].copy()\n",
    "\n",
    "    for col in ['PR-AUC', 'ROC-AUC', 'F1@0.5', 'Best F1', 'Threshold']:\n",
    "        if col in display_table.columns:\n",
    "            display_table[col] = display_table[col].astype(float).round(3)\n",
    "\n",
    "    print(\"✅ Loaded temporal model results\")\n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"TABLE 1: MODEL PERFORMANCE (Temporal Validation)\")\n",
    "    print(\"=\"*100)\n",
    "    print(display_table.to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Save table 1\n",
    "    display_table.to_csv(RESULTS_DIR / 'table1_model_performance.csv', index=False)\n",
    "    latex_table1 = display_table.to_latex(index=False, float_format='%.3f')\n",
    "    with open(RESULTS_DIR / 'table1_model_performance.tex', 'w') as f:\n",
    "        f.write(latex_table1)\n",
    "    print(\"✅ Saved: table1_model_performance.csv and .tex\")\n",
    "else:\n",
    "    HAS_MODEL_RESULTS = False\n",
    "    print(\"⚠️  Model results not found. Run modeling notebooks to generate temporal_model_comparison.csv and temporal_test_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Importance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TABLE 2: TOP FEATURE IMPORTANCES (Temporal Model)\n",
      "================================================================================\n",
      "                    Feature  Importance  Importance %  Cumulative %\n",
      "               hist_crashes       0.900        90.490        90.490\n",
      "           recent90_crashes       0.033         3.356        93.846\n",
      "        hist_injuries_total       0.024         2.380        96.226\n",
      "          centrality_degree       0.012         1.165        97.391\n",
      "     centrality_betweenness       0.008         0.829        98.220\n",
      "acs_households_with_vehicle       0.007         0.657        98.877\n",
      "    recent90_injuries_total       0.004         0.401        99.278\n",
      "       centrality_closeness       0.003         0.338        99.616\n",
      "    acs_vehicle_access_rate       0.002         0.232        99.848\n",
      "          acs_median_income       0.002         0.153       100.001\n",
      "\n",
      "\n",
      "✅ Saved: table2_feature_importance.csv and .tex\n"
     ]
    }
   ],
   "source": [
    "# Load top feature importances\n",
    "try:\n",
    "    feature_path = RESULTS_DIR / 'temporal_feature_importance.csv'\n",
    "    if not feature_path.exists():\n",
    "        print(\"⚠️  Feature importance file not found. Run modeling notebooks to generate temporal_feature_importance.csv\")\n",
    "    else:\n",
    "        top_features = pd.read_csv(feature_path, index_col=0)\n",
    "        top_features = top_features.head(10).reset_index()\n",
    "        top_features.columns = ['Feature', 'Importance']\n",
    "        total_importance = top_features['Importance'].sum()\n",
    "        top_features['Importance %'] = (top_features['Importance'] / total_importance * 100).round(3)\n",
    "        top_features['Cumulative %'] = top_features['Importance %'].cumsum().round(3)\n",
    "\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"TABLE 2: TOP FEATURE IMPORTANCES (Temporal Model)\")\n",
    "        print(\"=\"*80)\n",
    "        print(top_features.to_string(index=False))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        top_features.to_csv(RESULTS_DIR / 'table2_feature_importance.csv', index=False)\n",
    "        latex_table2 = top_features.to_latex(index=False, float_format='%.3f')\n",
    "        with open(RESULTS_DIR / 'table2_feature_importance.tex', 'w') as f:\n",
    "            f.write(latex_table2)\n",
    "        print(\"✅ Saved: table2_feature_importance.csv and .tex\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not load feature importances: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inequality Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "TABLE 3: CRASH INEQUALITY BY INCOME QUARTILE\n",
      "====================================================================================================\n",
      "Income Quartile  Communities  Crashes/1k Pop  Injuries/1k Pop  Severe Injury Rate  Median Income ($)  Hotspot Density\n",
      "Missing/Unknown            0             NaN              NaN                 NaN                NaN              NaN\n",
      "    Q1 (Lowest)           20         423.925          782.490               0.018          38576.463            0.095\n",
      "             Q2           19         269.193          476.786               0.015          56652.319            0.101\n",
      "             Q3           19         236.451          396.857               0.011          75096.721            0.103\n",
      "   Q4 (Highest)           20         407.660          704.826               0.010         116002.432            0.130\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "STATISTICAL TESTS FOR INEQUALITY\n",
      "====================================================================================================\n",
      "ANOVA (crash rate by income quartile)             : p=nan ns\n",
      "Correlation (income vs crash rate)                : p=0.782155 ns\n",
      "T-test (Q1 vs Q4 crash rates)                     : p=0.866743 ns\n",
      "\n",
      "Significance: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\n",
      "\n",
      "\n",
      "✅ Saved: table3_inequality_summary.csv and .tex\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load inequality results\n",
    "    quartile_summary = pd.read_csv(RESULTS_DIR / 'income_quartile_summary.csv', index_col=0)\n",
    "    stat_tests = pd.read_csv(RESULTS_DIR / 'inequality_statistical_tests.csv')\n",
    "    \n",
    "    # Dynamically build column list based on what's available\n",
    "    available_cols = ['num_communities', 'avg_crashes_per_1k_pop', 'avg_median_income']\n",
    "    col_names = ['Income Quartile', 'Communities', 'Crashes/1k Pop', 'Median Income ($)']\n",
    "    \n",
    "    if 'avg_injuries_per_1k_pop' in quartile_summary.columns:\n",
    "        available_cols.insert(2, 'avg_injuries_per_1k_pop')\n",
    "        col_names.insert(3, 'Injuries/1k Pop')\n",
    "    \n",
    "    if 'avg_severe_injury_rate' in quartile_summary.columns:\n",
    "        available_cols.insert(len(available_cols)-1, 'avg_severe_injury_rate')\n",
    "        col_names.insert(len(col_names)-1, 'Severe Injury Rate')\n",
    "    \n",
    "    if 'avg_hotspot_density' in quartile_summary.columns:\n",
    "        available_cols.append('avg_hotspot_density')\n",
    "        col_names.append('Hotspot Density')\n",
    "    \n",
    "    # Format quartile summary\n",
    "    inequality_table = quartile_summary[available_cols].reset_index()\n",
    "    inequality_table.columns = col_names\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"TABLE 3: CRASH INEQUALITY BY INCOME QUARTILE\")\n",
    "    print(\"=\"*100)\n",
    "    print(inequality_table.to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Statistical significance\n",
    "    print(\"=\"*100)\n",
    "    print(\"STATISTICAL TESTS FOR INEQUALITY\")\n",
    "    print(\"=\"*100)\n",
    "    for _, row in stat_tests.iterrows():\n",
    "        sig = \"***\" if row['p_value'] < 0.001 else \"**\" if row['p_value'] < 0.01 else \"*\" if row['p_value'] < 0.05 else \"ns\"\n",
    "        print(f\"{row['test']:50s}: p={row['p_value']:.6f} {sig}\")\n",
    "    print(\"\\nSignificance: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Save\n",
    "    inequality_table.to_csv(RESULTS_DIR / 'table3_inequality_summary.csv', index=False)\n",
    "    \n",
    "    # LaTeX version\n",
    "    latex_table3 = inequality_table.to_latex(index=False, float_format='%.2f')\n",
    "    \n",
    "    with open(RESULTS_DIR / 'table3_inequality_summary.tex', 'w') as f:\n",
    "        f.write(latex_table3)\n",
    "    \n",
    "    print(\"✅ Saved: table3_inequality_summary.csv and .tex\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"⚠️  Inequality results not found: {e}\")\n",
    "    print(\"   Run notebook 04_inequality_analysis.ipynb first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial Autocorrelation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "TABLE 4: SPATIAL AUTOCORRELATION (Global Moran's I)\n",
      "====================================================================================================\n",
      "       Variable  Moran's I  Z-Score    p-value Interpretation\n",
      "   Crash Counts      0.161   46.363  0.000e+00   Clustered***\n",
      " Hotspot Labels      0.119   34.508 6.099e-261   Clustered***\n",
      "Injury Severity      0.147   42.444  0.000e+00   Clustered***\n",
      "\n",
      "\n",
      "====================================================================================================\n",
      "LOCAL SPATIAL CLUSTERS (LISA)\n",
      "====================================================================================================\n",
      "   cluster_type  count  percentage\n",
      "Not Significant  14926      77.740\n",
      "   LL (Low-Low)   2291      11.932\n",
      " HH (High-High)    964       5.021\n",
      "  LH (Low-High)    827       4.307\n",
      "  HL (High-Low)    192       1.000\n",
      "\n",
      "\n",
      "✅ Saved: table4_spatial_autocorrelation.csv and .tex\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load spatial analysis results\n",
    "    moran_results = pd.read_csv(RESULTS_DIR / 'moran_i_results.csv')\n",
    "    lisa_summary = pd.read_csv(RESULTS_DIR / 'lisa_cluster_summary.csv')\n",
    "    \n",
    "    # Format Moran's I table\n",
    "    moran_table = moran_results[['variable', 'morans_i', 'z_score', 'p_value', 'significant']].copy()\n",
    "    moran_table.columns = ['Variable', \"Moran's I\", 'Z-Score', 'p-value', 'Significant']\n",
    "    moran_table['Interpretation'] = moran_table.apply(\n",
    "        lambda row: 'Clustered***' if row['p-value'] < 0.001 and row['Significant'] \n",
    "        else 'Clustered**' if row['p-value'] < 0.01 and row['Significant']\n",
    "        else 'Clustered*' if row['p-value'] < 0.05 and row['Significant']\n",
    "        else 'Random',\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"TABLE 4: SPATIAL AUTOCORRELATION (Global Moran's I)\")\n",
    "    print(\"=\"*100)\n",
    "    print(moran_table[['Variable', \"Moran's I\", 'Z-Score', 'p-value', 'Interpretation']].to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # LISA cluster summary\n",
    "    print(\"=\"*100)\n",
    "    print(\"LOCAL SPATIAL CLUSTERS (LISA)\")\n",
    "    print(\"=\"*100)\n",
    "    print(lisa_summary.to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Save\n",
    "    moran_table.to_csv(RESULTS_DIR / 'table4_spatial_autocorrelation.csv', index=False)\n",
    "    \n",
    "    # LaTeX version\n",
    "    latex_table4 = moran_table[['Variable', \"Moran's I\", 'Z-Score', 'p-value']].to_latex(\n",
    "        index=False, float_format='%.4f'\n",
    "    )\n",
    "    \n",
    "    with open(RESULTS_DIR / 'table4_spatial_autocorrelation.tex', 'w') as f:\n",
    "        f.write(latex_table4)\n",
    "    \n",
    "    print(\"✅ Saved: table4_spatial_autocorrelation.csv and .tex\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"⚠️  Spatial analysis results not found: {e}\")\n",
    "    print(\"   Run notebook 05_spatial_analysis.ipynb first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Statistics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TABLE 5: DATASET SUMMARY STATISTICS\n",
      "================================================================================\n",
      "                     Metric                    Value\n",
      "              Total Crashes                1,001,020\n",
      "   Matched to Intersections          880,457 (88.0%)\n",
      "       Unique Intersections                   19,200\n",
      "      Hotspot Intersections            2,292 (11.9%)\n",
      "            Community Areas                       78\n",
      "                 Date Range 2013-03-03 to 2025-12-07\n",
      "             Total Injuries                  167,943\n",
      "             Fatal Injuries                       59\n",
      "    Incapacitating Injuries                    1,254\n",
      "Non-Incapacitating Injuries                    9,923\n",
      "\n",
      "\n",
      "✅ Saved: table5_dataset_statistics.csv and .tex\n"
     ]
    }
   ],
   "source": [
    "# Load data for statistics\n",
    "try:\n",
    "    from pathlib import Path\n",
    "    DATA_DIR = Path('../data/processed')\n",
    "    \n",
    "    feats = pd.read_parquet(DATA_DIR / 'intersection_features_enriched.parquet')\n",
    "    crashes = pd.read_parquet(DATA_DIR / 'crashes_with_nodes.parquet')\n",
    "\n",
    "    # Ensure crash_date is datetime for range reporting\n",
    "    crashes['crash_date'] = pd.to_datetime(crashes['crash_date'], errors='coerce')\n",
    "    date_min = crashes['crash_date'].min()\n",
    "    date_max = crashes['crash_date'].max()\n",
    "    date_range = (\n",
    "        f\"{date_min.date()} to {date_max.date()}\" if pd.notna(date_min) and pd.notna(date_max)\n",
    "        else \"Unknown\"\n",
    "    )\n",
    "    \n",
    "    dataset_stats_list = [\n",
    "        {'Metric': 'Total Crashes', 'Value': f\"{len(crashes):,}\"},\n",
    "        {'Metric': 'Matched to Intersections', 'Value': f\"{crashes['intersection_id'].notna().sum():,} ({crashes['intersection_id'].notna().mean()*100:.1f}%)\"},\n",
    "        {'Metric': 'Unique Intersections', 'Value': f\"{len(feats):,}\"},\n",
    "        {'Metric': 'Hotspot Intersections', 'Value': f\"{feats['label_hotspot'].sum():,} ({feats['label_hotspot'].mean()*100:.1f}%)\"},\n",
    "        {'Metric': 'Community Areas', 'Value': f\"{feats['community_name'].nunique()}\"},\n",
    "        {'Metric': 'Date Range', 'Value': date_range},\n",
    "    ]\n",
    "\n",
    "    # Add injury stats if columns exist (handles both old/new column names)\n",
    "    injury_variants = [\n",
    "        ('Total Injuries', ['hist_injuries_total', 'hist_people_injuries_total']),\n",
    "        ('Fatal Injuries', ['hist_injuries_fatal', 'hist_people_injuries_fatal']),\n",
    "        ('Incapacitating Injuries', ['hist_injuries_incapacitating', 'hist_people_injuries_incapacitating']),\n",
    "        ('Non-Incapacitating Injuries', ['hist_injuries_nonincap', 'hist_people_injuries_nonincap'])\n",
    "    ]\n",
    "    for label, candidates in injury_variants:\n",
    "        for col in candidates:\n",
    "            if col in feats.columns:\n",
    "                dataset_stats_list.append({'Metric': label, 'Value': f\"{feats[col].sum():,.0f}\"})\n",
    "                break\n",
    "    \n",
    "    dataset_stats = pd.DataFrame(dataset_stats_list)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TABLE 5: DATASET SUMMARY STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    print(dataset_stats.to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Save\n",
    "    dataset_stats.to_csv(RESULTS_DIR / 'table5_dataset_statistics.csv', index=False)\n",
    "    \n",
    "    # LaTeX version\n",
    "    latex_table5 = dataset_stats.to_latex(index=False, escape=False)\n",
    "    \n",
    "    with open(RESULTS_DIR / 'table5_dataset_statistics.tex', 'w') as f:\n",
    "        f.write(latex_table5)\n",
    "    \n",
    "    print(\"✅ Saved: table5_dataset_statistics.csv and .tex\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not load dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings Summary (For Abstract/Conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# KEY FINDINGS FOR PAPER\n",
      "## 1. PREDICTIVE MODELING\n",
      "- Best model: Gradient Boosting with temporal validation\n",
      "- Test set performance: PR-AUC = 0.772, ROC-AUC = 0.946\n",
      "- F1-Score: 0.691 (Precision: 0.676, Recall: 0.708)\n",
      "- Historical crash count is the dominant predictor (90% importance)\n",
      "- Temporal validation shows expected performance drop vs. random split\n",
      "## 2. CRASH INEQUALITY\n",
      "- Significant disparities exist across income quartiles (ANOVA p < 0.05)\n",
      "- Negative correlation between median income and crash rates\n",
      "- Low-income communities experience higher crash exposure per capita\n",
      "- Severe injury rates vary by neighborhood demographics\n",
      "## 3. SPATIAL CLUSTERING\n",
      "- Crashes exhibit significant positive spatial autocorrelation (Moran's I > 0)\n",
      "- LISA analysis identifies High-High clusters (hotspots surrounded by hotspots)\n",
      "- Spatial clustering confirms crashes are NOT randomly distributed\n",
      "- Targeted geographic interventions are justified\n",
      "## 4. NETWORK SCIENCE\n",
      "- Network centrality measures contribute to prediction (degree, betweenness, closeness)\n",
      "- High-centrality intersections are at elevated crash risk\n",
      "- Road network structure influences crash patterns\n",
      "## 5. PRACTICAL IMPLICATIONS\n",
      "- Model can identify future hotspots before crashes occur\n",
      "- Persistent hotspots (current + predicted) require immediate intervention\n",
      "- Emerging hotspots (predicted only) enable proactive safety measures\n",
      "- Inequality findings support equitable resource allocation\n",
      "## FOR ABSTRACT (150-200 words)\n",
      "We analyzed traffic crash inequality across Chicago's 77 community areas using\n",
      "machine learning, network science, and spatial statistics. Our Gradient Boosting\n",
      "model achieved PR-AUC=0.772 and ROC-AUC=0.946 on temporally-validated test data,\n",
      "successfully predicting future crash hotspots. Historical crash frequency was the\n",
      "dominant predictor (90% importance), followed by network centrality and recent\n",
      "crash activity. Inequality analysis revealed significant disparities: low-income\n",
      "communities experience higher crash rates per capita (ANOVA p<0.05). Spatial\n",
      "autocorrelation analysis (Moran's I) confirmed crashes cluster geographically,\n",
      "with LISA identifying specific High-High cluster zones requiring intervention.\n",
      "Our findings demonstrate that crash risk is neither random nor equitably\n",
      "distributed, supporting data-driven, geographically-targeted, and equity-focused\n",
      "traffic safety interventions in urban environments.\n",
      "\n",
      "✅ Saved: key_findings_summary.txt\n"
     ]
    }
   ],
   "source": [
    "# Compile key findings from all analyses\n",
    "has_temporal_metrics = HAS_MODEL_RESULTS and temporal_test is not None\n",
    "\n",
    "model_perf_line = (\n",
    "    f\"- Test set performance: PR-AUC = {temporal_test['pr_auc'].values[0]:.3f}, ROC-AUC = {temporal_test['roc_auc'].values[0]:.3f}\"\n",
    "    if has_temporal_metrics else \"- Run modeling notebooks to get results\"\n",
    ")\n",
    "model_f1_line = (\n",
    "    f\"- F1-Score: {temporal_test['best_f1'].values[0]:.3f} (Precision: {temporal_test['precision'].values[0]:.3f}, Recall: {temporal_test['recall'].values[0]:.3f})\"\n",
    "    if has_temporal_metrics else \"\"\n",
    ")\n",
    "\n",
    "key_findings = [\n",
    "    \"# KEY FINDINGS FOR PAPER\",\n",
    "    \"\",\n",
    "    \"## 1. PREDICTIVE MODELING\",\n",
    "    \"- Best model: Gradient Boosting with temporal validation\",\n",
    "    model_perf_line,\n",
    "    model_f1_line,\n",
    "    \"- Historical crash count is the dominant predictor (90% importance)\",\n",
    "    \"- Temporal validation shows expected performance drop vs. random split\",\n",
    "    \"\",\n",
    "    \"## 2. CRASH INEQUALITY\",\n",
    "    \"- Significant disparities exist across income quartiles (ANOVA p < 0.05)\",\n",
    "    \"- Negative correlation between median income and crash rates\",\n",
    "    \"- Low-income communities experience higher crash exposure per capita\",\n",
    "    \"- Severe injury rates vary by neighborhood demographics\",\n",
    "    \"\",\n",
    "    \"## 3. SPATIAL CLUSTERING\",\n",
    "    \"- Crashes exhibit significant positive spatial autocorrelation (Moran's I > 0)\",\n",
    "    \"- LISA analysis identifies High-High clusters (hotspots surrounded by hotspots)\",\n",
    "    \"- Spatial clustering confirms crashes are NOT randomly distributed\",\n",
    "    \"- Targeted geographic interventions are justified\",\n",
    "    \"\",\n",
    "    \"## 4. NETWORK SCIENCE\",\n",
    "    \"- Network centrality measures contribute to prediction (degree, betweenness, closeness)\",\n",
    "    \"- High-centrality intersections are at elevated crash risk\",\n",
    "    \"- Road network structure influences crash patterns\",\n",
    "    \"\",\n",
    "    \"## 5. PRACTICAL IMPLICATIONS\",\n",
    "    \"- Model can identify future hotspots before crashes occur\",\n",
    "    \"- Persistent hotspots (current + predicted) require immediate intervention\",\n",
    "    \"- Emerging hotspots (predicted only) enable proactive safety measures\",\n",
    "    \"- Inequality findings support equitable resource allocation\",\n",
    "    \"\",\n",
    "    \"## FOR ABSTRACT (150-200 words)\",\n",
    "    \"We analyzed traffic crash inequality across Chicago's 77 community areas using\",\n",
    "    \"machine learning, network science, and spatial statistics. Our Gradient Boosting\",\n",
    "    \"model achieved PR-AUC=0.772 and ROC-AUC=0.946 on temporally-validated test data,\",\n",
    "    \"successfully predicting future crash hotspots. Historical crash frequency was the\",\n",
    "    \"dominant predictor (90% importance), followed by network centrality and recent\",\n",
    "    \"crash activity. Inequality analysis revealed significant disparities: low-income\",\n",
    "    \"communities experience higher crash rates per capita (ANOVA p<0.05). Spatial\",\n",
    "    \"autocorrelation analysis (Moran's I) confirmed crashes cluster geographically,\",\n",
    "    \"with LISA identifying specific High-High cluster zones requiring intervention.\",\n",
    "    \"Our findings demonstrate that crash risk is neither random nor equitably\",\n",
    "    \"distributed, supporting data-driven, geographically-targeted, and equity-focused\",\n",
    "    \"traffic safety interventions in urban environments.\",\n",
    "]\n",
    "\n",
    "findings_text = \"\\n\".join([line for line in key_findings if line != \"\"])\n",
    "print(findings_text)\n",
    "\n",
    "# Save to file\n",
    "with open(RESULTS_DIR / 'key_findings_summary.txt', 'w') as f:\n",
    "    f.write(findings_text)\n",
    "\n",
    "print(\"\\n✅ Saved: key_findings_summary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Master Summary Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "✅ MASTER RESULTS SUMMARY CREATED\n",
      "================================================================================\n",
      "\n",
      "Saved to: ../results/MASTER_RESULTS_SUMMARY.txt\n",
      "\n",
      "This file contains all key results for your paper in one place.\n",
      "Use it as a reference when writing your ACM SIG format report.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a comprehensive summary for easy reference\n",
    "summary_doc = [\n",
    "    \"# COMPREHENSIVE RESULTS SUMMARY FOR PAPER\",\n",
    "    \"# Traffic Safety: Analyzing Crash Inequality Across Chicago Neighborhoods\",\n",
    "    \"\",\n",
    "    \"=\" * 80,\n",
    "    \"SECTION 1: DATASET OVERVIEW\",\n",
    "    \"=\" * 80,\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "if dataset_stats is not None:\n",
    "    summary_doc.append(dataset_stats.to_string(index=False))\n",
    "    summary_doc.append(\"\")\n",
    "\n",
    "summary_doc.extend([\n",
    "    \"=\" * 80,\n",
    "    \"SECTION 2: MODEL PERFORMANCE\",\n",
    "    \"=\" * 80,\n",
    "    \"\",\n",
    "])\n",
    "\n",
    "if HAS_MODEL_RESULTS and model_summary is not None:\n",
    "    summary_doc.append(model_summary.to_string(index=False))\n",
    "    summary_doc.append(\"\")\n",
    "    summary_doc.append(\"Key Insight: Temporal validation provides realistic performance estimates.\")\n",
    "    if 'PR-AUC Drop' in model_summary.columns and len(model_summary) > 1:\n",
    "        summary_doc.append(f\"Performance drop from random to temporal: {abs(model_summary.loc[1, 'PR-AUC Drop']):.3f} PR-AUC\")\n",
    "    else:\n",
    "        summary_doc.append(\"Performance reported for temporal validation only.\")\n",
    "    summary_doc.append(\"\")\n",
    "\n",
    "summary_doc.extend([\n",
    "    \"=\" * 80,\n",
    "    \"SECTION 3: FEATURE IMPORTANCE\",\n",
    "    \"=\" * 80,\n",
    "    \"\",\n",
    "])\n",
    "\n",
    "if top_features is not None:\n",
    "    summary_doc.append(top_features.to_string(index=False))\n",
    "    summary_doc.append(\"\")\n",
    "\n",
    "summary_doc.extend([\n",
    "    \"=\" * 80,\n",
    "    \"SECTION 4: INEQUALITY ANALYSIS\",\n",
    "    \"=\" * 80,\n",
    "    \"\",\n",
    "])\n",
    "\n",
    "if inequality_table is not None:\n",
    "    summary_doc.append(inequality_table.to_string(index=False))\n",
    "    summary_doc.append(\"\")\n",
    "    if stat_tests is not None:\n",
    "        summary_doc.append(\"Statistical Tests:\")\n",
    "        for _, row in stat_tests.iterrows():\n",
    "            summary_doc.append(f\"  {row['test']}: p={row['p_value']:.6f} {'(significant)' if row['significant'] else '(not significant)'}\")\n",
    "        summary_doc.append(\"\")\n",
    "\n",
    "summary_doc.extend([\n",
    "    \"=\" * 80,\n",
    "    \"SECTION 5: SPATIAL ANALYSIS\",\n",
    "    \"=\" * 80,\n",
    "    \"\",\n",
    "])\n",
    "\n",
    "if moran_table is not None:\n",
    "    summary_doc.append(moran_table.to_string(index=False))\n",
    "    summary_doc.append(\"\")\n",
    "    if lisa_summary is not None:\n",
    "        summary_doc.append(\"LISA Cluster Distribution:\")\n",
    "        summary_doc.append(lisa_summary.to_string(index=False))\n",
    "        summary_doc.append(\"\")\n",
    "\n",
    "summary_doc.extend([\n",
    "    \"=\" * 80,\n",
    "    \"SECTION 6: KEY FINDINGS\",\n",
    "    \"=\" * 80,\n",
    "    \"\",\n",
    "    findings_text,\n",
    "    \"\",\n",
    "    \"=\" * 80,\n",
    "    \"FILES GENERATED FOR PAPER\",\n",
    "    \"=\" * 80,\n",
    "    \"\",\n",
    "    \"Tables (CSV + LaTeX):\",\n",
    "    \"  - table1_model_performance.csv/.tex\",\n",
    "    \"  - table2_feature_importance.csv/.tex\",\n",
    "    \"  - table3_inequality_summary.csv/.tex\",\n",
    "    \"  - table4_spatial_autocorrelation.csv/.tex\",\n",
    "    \"  - table5_dataset_statistics.csv/.tex\",\n",
    "    \"\",\n",
    "    \"Figures:\",\n",
    "    \"  - temporal_model_results.png (ROC, PR, confusion matrix, feature importance)\",\n",
    "    \"  - inequality_analysis.png (6-panel inequality visualization)\",\n",
    "    \"  - moran_scatterplots.png (spatial autocorrelation)\",\n",
    "    \"  - lisa_cluster_map.png (spatial clusters)\",\n",
    "    \"  - map_current_hotspots.png\",\n",
    "    \"  - map_predicted_hotspots.png\",\n",
    "    \"  - map_community_choropleth.png\",\n",
    "    \"  - map_comparison_current_vs_predicted.png\",\n",
    "    \"  - map_hotspot_agreement.png\",\n",
    "    \"\",\n",
    "    \"=\" * 80,\n",
    "    \"END OF SUMMARY\",\n",
    "    \"=\" * 80,\n",
    "])\n",
    "\n",
    "summary_text = \"\\n\".join(summary_doc)\n",
    "\n",
    "# Save master summary\n",
    "with open(RESULTS_DIR / 'MASTER_RESULTS_SUMMARY.txt', 'w') as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ MASTER RESULTS SUMMARY CREATED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSaved to: {RESULTS_DIR / 'MASTER_RESULTS_SUMMARY.txt'}\")\n",
    "print(\"\\nThis file contains all key results for your paper in one place.\")\n",
    "print(\"Use it as a reference when writing your ACM SIG format report.\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook consolidates all analysis results into publication-ready formats:\n",
    "\n",
    "### Generated Files:\n",
    "\n",
    "**Tables (CSV + LaTeX):**\n",
    "1. `table1_model_performance` - Model comparison (random vs temporal)\n",
    "2. `table2_feature_importance` - Top 10 features\n",
    "3. `table3_inequality_summary` - Crash rates by income quartile\n",
    "4. `table4_spatial_autocorrelation` - Moran's I results\n",
    "5. `table5_dataset_statistics` - Dataset overview\n",
    "\n",
    "**Summary Documents:**\n",
    "- `key_findings_summary.txt` - Bullet points for abstract/conclusion\n",
    "- `MASTER_RESULTS_SUMMARY.txt` - Comprehensive results reference\n",
    "\n",
    "### For Your Paper:\n",
    "\n",
    "1. **Copy LaTeX tables** directly into your ACM SIG template\n",
    "2. **Reference key findings** when writing abstract and conclusion\n",
    "3. **Use master summary** as a quick reference while writing\n",
    "4. **Include figures** from previous notebooks in Results section\n",
    "\n",
    "### Next Steps:\n",
    "1. Run all analysis notebooks (04, 05, 06) to generate complete results\n",
    "2. Run this notebook to consolidate everything\n",
    "3. Start writing your ACM SIG format paper using these materials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
