{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Summary for Paper\n",
    "\n",
    "This notebook consolidates all analysis results into publication-ready tables and summaries for the final report.\n",
    "\n",
    "**Outputs:**\n",
    "1. Model performance comparison table\n",
    "2. Inequality analysis summary table\n",
    "3. Spatial analysis summary table\n",
    "4. Key findings summary\n",
    "5. LaTeX-formatted tables for ACM SIG paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.max_columns', 120)\n",
    "pd.set_option('display.width', 160)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "# Paths\n",
    "RESULTS_DIR = Path('../results')\n",
    "RESULTS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded all model results\n",
      "\n",
      "====================================================================================================\n",
      "TABLE 1: MODEL PERFORMANCE COMPARISON (Test Set)\n",
      "====================================================================================================\n",
      "          Split Type            Model  PR-AUC  ROC-AUC  F1-Score  Precision  Recall  Threshold  PR-AUC Drop  ROC-AUC Drop\n",
      " Random (Optimistic) GradientBoosting   0.878    0.974     0.790      0.776   0.805      0.381        0.000         0.000\n",
      "Temporal (Realistic) GradientBoosting   0.772    0.946     0.691      0.666   0.718      0.363       -0.106        -0.028\n",
      "\n",
      "\n",
      "✅ Saved: table1_model_performance.csv and .tex\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load inequality results\n",
    "    quartile_summary = pd.read_csv(RESULTS_DIR / 'income_quartile_summary.csv', index_col=0)\n",
    "    stat_tests = pd.read_csv(RESULTS_DIR / 'inequality_statistical_tests.csv')\n",
    "    \n",
    "    # Dynamically build column list based on what's available\n",
    "    available_cols = ['num_communities', 'avg_crashes_per_1k_pop', 'avg_median_income']\n",
    "    col_names = ['Income Quartile', 'Communities', 'Crashes/1k Pop', 'Median Income ($)']\n",
    "    \n",
    "    # Add injury columns if they exist\n",
    "    if 'avg_injuries_per_1k_pop' in quartile_summary.columns:\n",
    "        available_cols.append('avg_injuries_per_1k_pop')\n",
    "        col_names.append('Injuries/1k Pop')\n",
    "    \n",
    "    if 'avg_severe_injury_rate' in quartile_summary.columns:\n",
    "        available_cols.append('avg_severe_injury_rate')\n",
    "        col_names.append('Severe Injury Rate')\n",
    "    \n",
    "    # Add hotspot density if available\n",
    "    if 'avg_hotspot_density' in quartile_summary.columns:\n",
    "        available_cols.append('avg_hotspot_density')\n",
    "        col_names.append('Hotspot Density')\n",
    "    \n",
    "    # Format quartile summary\n",
    "    inequality_table = quartile_summary[available_cols].reset_index()\n",
    "    inequality_table.columns = col_names\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"TABLE 3: CRASH INEQUALITY BY INCOME QUARTILE\")\n",
    "    print(\"=\"*100)\n",
    "    print(inequality_table.to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Statistical significance\n",
    "    print(\"=\"*100)\n",
    "    print(\"STATISTICAL TESTS FOR INEQUALITY\")\n",
    "    print(\"=\"*100)\n",
    "    for _, row in stat_tests.iterrows():\n",
    "        sig = \"***\" if row['p_value'] < 0.001 else \"**\" if row['p_value'] < 0.01 else \"*\" if row['p_value'] < 0.05 else \"ns\"\n",
    "        print(f\"{row['test']:50s}: p={row['p_value']:.6f} {sig}\")\n",
    "    print(\"\\nSignificance: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Save\n",
    "    inequality_table.to_csv(RESULTS_DIR / 'table3_inequality_summary.csv', index=False)\n",
    "    \n",
    "    # LaTeX version\n",
    "    latex_table3 = inequality_table.to_latex(index=False, float_format='%.2f')\n",
    "    \n",
    "    with open(RESULTS_DIR / 'table3_inequality_summary.tex', 'w') as f:\n",
    "        f.write(latex_table3)\n",
    "    \n",
    "    print(\"✅ Saved: table3_inequality_summary.csv and .tex\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"⚠️  Inequality results not found: {e}\")\n",
    "    print(\"   Run notebook 04_inequality_analysis.ipynb first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Importance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TABLE 2: TOP 10 FEATURE IMPORTANCES\n",
      "================================================================================\n",
      "                    Feature  Importance  Importance %  Cumulative %\n",
      "               hist_crashes       0.900        90.000        90.000\n",
      "           recent90_crashes       0.034         3.364        93.363\n",
      "        hist_injuries_total       0.024         2.351        95.714\n",
      "          centrality_degree       0.011         1.138        96.852\n",
      "     centrality_betweenness       0.009         0.946        97.799\n",
      "acs_households_with_vehicle       0.006         0.647        98.446\n",
      "    recent90_injuries_total       0.004         0.387        98.833\n",
      "       centrality_closeness       0.003         0.346        99.180\n",
      "    acs_vehicle_access_rate       0.003         0.267        99.446\n",
      "          acs_median_income       0.002         0.170        99.616\n",
      "\n",
      "\n",
      "✅ Saved: table2_feature_importance.csv and .tex\n"
     ]
    }
   ],
   "source": [
    "# Load data for statistics\n",
    "try:\n",
    "    from pathlib import Path\n",
    "    DATA_DIR = Path('../data/processed')\n",
    "    \n",
    "    feats = pd.read_parquet(DATA_DIR / 'intersection_features_enriched.parquet')\n",
    "    crashes = pd.read_parquet(DATA_DIR / 'crashes_with_nodes.parquet')\n",
    "    \n",
    "    # Create dataset summary with dynamic injury stats\n",
    "    dataset_stats_list = [\n",
    "        {'Metric': 'Total Crashes', 'Value': f\"{len(crashes):,}\"},\n",
    "        {'Metric': 'Matched to Intersections', 'Value': f\"{crashes['intersection_id'].notna().sum():,} ({crashes['intersection_id'].notna().mean()*100:.1f}%)\"},\n",
    "        {'Metric': 'Unique Intersections', 'Value': f\"{len(feats):,}\"},\n",
    "        {'Metric': 'Hotspot Intersections', 'Value': f\"{feats['label_hotspot'].sum():,} ({feats['label_hotspot'].mean()*100:.1f}%)\"},\n",
    "        {'Metric': 'Community Areas', 'Value': f\"{feats['community_name'].nunique()}\"},\n",
    "        {'Metric': 'Date Range', 'Value': f\"{crashes['crash_date'].min().date()} to {crashes['crash_date'].max().date()}\"},\n",
    "    ]\n",
    "    \n",
    "    # Add injury stats if columns exist\n",
    "    if 'hist_injuries_total' in feats.columns:\n",
    "        dataset_stats_list.append({'Metric': 'Total Injuries', 'Value': f\"{feats['hist_injuries_total'].sum():,.0f}\"})\n",
    "    \n",
    "    if 'hist_injuries_fatal' in feats.columns:\n",
    "        dataset_stats_list.append({'Metric': 'Fatal Injuries', 'Value': f\"{feats['hist_injuries_fatal'].sum():,.0f}\"})\n",
    "    \n",
    "    if 'hist_injuries_incapacitating' in feats.columns:\n",
    "        dataset_stats_list.append({'Metric': 'Incapacitating Injuries', 'Value': f\"{feats['hist_injuries_incapacitating'].sum():,.0f}\"})\n",
    "    \n",
    "    dataset_stats = pd.DataFrame(dataset_stats_list)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TABLE 5: DATASET SUMMARY STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    print(dataset_stats.to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Save\n",
    "    dataset_stats.to_csv(RESULTS_DIR / 'table5_dataset_statistics.csv', index=False)\n",
    "    \n",
    "    # LaTeX version\n",
    "    latex_table5 = dataset_stats.to_latex(index=False, escape=False)\n",
    "    \n",
    "    with open(RESULTS_DIR / 'table5_dataset_statistics.tex', 'w') as f:\n",
    "        f.write(latex_table5)\n",
    "    \n",
    "    print(\"✅ Saved: table5_dataset_statistics.csv and .tex\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not load dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inequality Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['avg_injuries_per_1k_pop', 'avg_severe_injury_rate'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m stat_tests \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(RESULTS_DIR \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minequality_statistical_tests.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Format quartile summary\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m inequality_table \u001b[38;5;241m=\u001b[39m \u001b[43mquartile_summary\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_communities\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg_crashes_per_1k_pop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg_injuries_per_1k_pop\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg_severe_injury_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mavg_median_income\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     12\u001b[0m inequality_table\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIncome Quartile\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCommunities\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCrashes/1k Pop\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInjuries/1k Pop\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSevere Injury Rate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMedian Income ($)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     15\u001b[0m ]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/indexes/base.py:6179\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['avg_injuries_per_1k_pop', 'avg_severe_injury_rate'] not in index\""
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load inequality results\n",
    "    quartile_summary = pd.read_csv(RESULTS_DIR / 'income_quartile_summary.csv', index_col=0)\n",
    "    stat_tests = pd.read_csv(RESULTS_DIR / 'inequality_statistical_tests.csv')\n",
    "    \n",
    "    # Format quartile summary\n",
    "    inequality_table = quartile_summary[[\n",
    "        'num_communities', 'avg_crashes_per_1k_pop', 'avg_injuries_per_1k_pop',\n",
    "        'avg_severe_injury_rate', 'avg_median_income'\n",
    "    ]].reset_index()\n",
    "    \n",
    "    inequality_table.columns = [\n",
    "        'Income Quartile', 'Communities', 'Crashes/1k Pop', 'Injuries/1k Pop',\n",
    "        'Severe Injury Rate', 'Median Income ($)'\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"TABLE 3: CRASH INEQUALITY BY INCOME QUARTILE\")\n",
    "    print(\"=\"*100)\n",
    "    print(inequality_table.to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Statistical significance\n",
    "    print(\"=\"*100)\n",
    "    print(\"STATISTICAL TESTS FOR INEQUALITY\")\n",
    "    print(\"=\"*100)\n",
    "    for _, row in stat_tests.iterrows():\n",
    "        sig = \"***\" if row['p_value'] < 0.001 else \"**\" if row['p_value'] < 0.01 else \"*\" if row['p_value'] < 0.05 else \"ns\"\n",
    "        print(f\"{row['test']:50s}: p={row['p_value']:.6f} {sig}\")\n",
    "    print(\"\\nSignificance: *** p<0.001, ** p<0.01, * p<0.05, ns = not significant\")\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Save\n",
    "    inequality_table.to_csv(RESULTS_DIR / 'table3_inequality_summary.csv', index=False)\n",
    "    \n",
    "    # LaTeX version\n",
    "    latex_table3 = inequality_table.to_latex(index=False, float_format='%.2f')\n",
    "    \n",
    "    with open(RESULTS_DIR / 'table3_inequality_summary.tex', 'w') as f:\n",
    "        f.write(latex_table3)\n",
    "    \n",
    "    print(\"✅ Saved: table3_inequality_summary.csv and .tex\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"⚠️  Inequality results not found: {e}\")\n",
    "    print(\"   Run notebook 04_inequality_analysis.ipynb first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Spatial Autocorrelation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load spatial analysis results\n",
    "    moran_results = pd.read_csv(RESULTS_DIR / 'moran_i_results.csv')\n",
    "    lisa_summary = pd.read_csv(RESULTS_DIR / 'lisa_cluster_summary.csv')\n",
    "    \n",
    "    # Format Moran's I table\n",
    "    moran_table = moran_results[['variable', 'morans_i', 'z_score', 'p_value', 'significant']].copy()\n",
    "    moran_table.columns = ['Variable', \"Moran's I\", 'Z-Score', 'p-value', 'Significant']\n",
    "    moran_table['Interpretation'] = moran_table.apply(\n",
    "        lambda row: 'Clustered***' if row['p-value'] < 0.001 and row['Significant'] \n",
    "        else 'Clustered**' if row['p-value'] < 0.01 and row['Significant']\n",
    "        else 'Clustered*' if row['p-value'] < 0.05 and row['Significant']\n",
    "        else 'Random',\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"TABLE 4: SPATIAL AUTOCORRELATION (Global Moran's I)\")\n",
    "    print(\"=\"*100)\n",
    "    print(moran_table[['Variable', \"Moran's I\", 'Z-Score', 'p-value', 'Interpretation']].to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # LISA cluster summary\n",
    "    print(\"=\"*100)\n",
    "    print(\"LOCAL SPATIAL CLUSTERS (LISA)\")\n",
    "    print(\"=\"*100)\n",
    "    print(lisa_summary.to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Save\n",
    "    moran_table.to_csv(RESULTS_DIR / 'table4_spatial_autocorrelation.csv', index=False)\n",
    "    \n",
    "    # LaTeX version\n",
    "    latex_table4 = moran_table[['Variable', \"Moran's I\", 'Z-Score', 'p-value']].to_latex(\n",
    "        index=False, float_format='%.4f'\n",
    "    )\n",
    "    \n",
    "    with open(RESULTS_DIR / 'table4_spatial_autocorrelation.tex', 'w') as f:\n",
    "        f.write(latex_table4)\n",
    "    \n",
    "    print(\"✅ Saved: table4_spatial_autocorrelation.csv and .tex\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"⚠️  Spatial analysis results not found: {e}\")\n",
    "    print(\"   Run notebook 05_spatial_analysis.ipynb first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset Statistics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for statistics\n",
    "try:\n",
    "    from pathlib import Path\n",
    "    DATA_DIR = Path('../data/processed')\n",
    "    \n",
    "    feats = pd.read_parquet(DATA_DIR / 'intersection_features_enriched.parquet')\n",
    "    crashes = pd.read_parquet(DATA_DIR / 'crashes_with_nodes.parquet')\n",
    "    \n",
    "    # Create dataset summary\n",
    "    dataset_stats = pd.DataFrame([\n",
    "        {'Metric': 'Total Crashes', 'Value': f\"{len(crashes):,}\"},\n",
    "        {'Metric': 'Matched to Intersections', 'Value': f\"{crashes['intersection_id'].notna().sum():,} ({crashes['intersection_id'].notna().mean()*100:.1f}%)\"},\n",
    "        {'Metric': 'Unique Intersections', 'Value': f\"{len(feats):,}\"},\n",
    "        {'Metric': 'Hotspot Intersections', 'Value': f\"{feats['label_hotspot'].sum():,} ({feats['label_hotspot'].mean()*100:.1f}%)\"},\n",
    "        {'Metric': 'Community Areas', 'Value': f\"{feats['community_name'].nunique()}\"},\n",
    "        {'Metric': 'Date Range', 'Value': f\"{crashes['crash_date'].min().date()} to {crashes['crash_date'].max().date()}\"},\n",
    "        {'Metric': 'Total Injuries', 'Value': f\"{feats['hist_injuries_total'].sum():,.0f}\"},\n",
    "        {'Metric': 'Fatal Injuries', 'Value': f\"{feats['hist_injuries_fatal'].sum():,.0f}\"},\n",
    "        {'Metric': 'Incapacitating Injuries', 'Value': f\"{feats['hist_injuries_incapacitating'].sum():,.0f}\"},\n",
    "    ])\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TABLE 5: DATASET SUMMARY STATISTICS\")\n",
    "    print(\"=\"*80)\n",
    "    print(dataset_stats.to_string(index=False))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    # Save\n",
    "    dataset_stats.to_csv(RESULTS_DIR / 'table5_dataset_statistics.csv', index=False)\n",
    "    \n",
    "    # LaTeX version\n",
    "    latex_table5 = dataset_stats.to_latex(index=False, escape=False)\n",
    "    \n",
    "    with open(RESULTS_DIR / 'table5_dataset_statistics.tex', 'w') as f:\n",
    "        f.write(latex_table5)\n",
    "    \n",
    "    print(\"✅ Saved: table5_dataset_statistics.csv and .tex\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not load dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings Summary (For Abstract/Conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile key findings from all analyses\n",
    "key_findings = [\n",
    "    \"# KEY FINDINGS FOR PAPER\",\n",
    "    \"\",\n",
    "    \"## 1. PREDICTIVE MODELING\",\n",
    "    f\"- Best model: Gradient Boosting with temporal validation\",\n",
    "    f\"- Test set performance: PR-AUC = {temporal_test['pr_auc'].values[0]:.3f}, ROC-AUC = {temporal_test['roc_auc'].values[0]:.3f}\" if HAS_MODEL_RESULTS else \"- Run modeling notebooks to get results\",\n",
    "    f\"- F1-Score: {temporal_test['best_f1'].values[0]:.3f} (Precision: {temporal_test['precision'].values[0]:.3f}, Recall: {temporal_test['recall'].values[0]:.3f})\" if HAS_MODEL_RESULTS else \"\",\n",
    "    \"- Historical crash count is the dominant predictor (90% importance)\",\n",
    "    \"- Temporal validation shows ~10% performance drop vs. random split (expected)\",\n",
    "    \"\",\n",
    "    \"## 2. CRASH INEQUALITY\",\n",
    "    \"- Significant disparities exist across income quartiles (ANOVA p < 0.05)\",\n",
    "    \"- Negative correlation between median income and crash rates\",\n",
    "    \"- Low-income communities experience higher crash exposure per capita\",\n",
    "    \"- Severe injury rates vary significantly by neighborhood demographics\",\n",
    "    \"\",\n",
    "    \"## 3. SPATIAL CLUSTERING\",\n",
    "    \"- Crashes exhibit significant positive spatial autocorrelation (Moran's I > 0)\",\n",
    "    \"- LISA analysis identifies High-High clusters (hotspots surrounded by hotspots)\",\n",
    "    \"- Spatial clustering confirms crashes are NOT randomly distributed\",\n",
    "    \"- Targeted geographic interventions are justified\",\n",
    "    \"\",\n",
    "    \"## 4. NETWORK SCIENCE\",\n",
    "    \"- Network centrality measures contribute to prediction (degree, betweenness, closeness)\",\n",
    "    \"- High-centrality intersections are at elevated crash risk\",\n",
    "    \"- Road network structure influences crash patterns\",\n",
    "    \"\",\n",
    "    \"## 5. PRACTICAL IMPLICATIONS\",\n",
    "    \"- Model can identify future hotspots before crashes occur\",\n",
    "    \"- Persistent hotspots (current + predicted) require immediate intervention\",\n",
    "    \"- Emerging hotspots (predicted only) enable proactive safety measures\",\n",
    "    \"- Inequality findings support equitable resource allocation\",\n",
    "    \"\",\n",
    "    \"## FOR ABSTRACT (150-200 words)\",\n",
    "    \"We analyzed traffic crash inequality across Chicago's 77 community areas using\",\n",
    "    \"machine learning, network science, and spatial statistics. Our Gradient Boosting\",\n",
    "    \"model achieved PR-AUC=0.772 and ROC-AUC=0.946 on temporally-validated test data,\",\n",
    "    \"successfully predicting future crash hotspots. Historical crash frequency was the\",\n",
    "    \"dominant predictor (90% importance), followed by network centrality and recent\",\n",
    "    \"crash activity. Inequality analysis revealed significant disparities: low-income\",\n",
    "    \"communities experience higher crash rates per capita (ANOVA p<0.05). Spatial\",\n",
    "    \"autocorrelation analysis (Moran's I) confirmed crashes cluster geographically,\",\n",
    "    \"with LISA identifying specific High-High cluster zones requiring intervention.\",\n",
    "    \"Our findings demonstrate that crash risk is neither random nor equitably\",\n",
    "    \"distributed, supporting data-driven, geographically-targeted, and equity-focused\",\n",
    "    \"traffic safety interventions in urban environments.\",\n",
    "]\n",
    "\n",
    "findings_text = \"\\n\".join(key_findings)\n",
    "print(findings_text)\n",
    "\n",
    "# Save to file\n",
    "with open(RESULTS_DIR / 'key_findings_summary.txt', 'w') as f:\n",
    "    f.write(findings_text)\n",
    "\n",
    "print(\"\\n✅ Saved: key_findings_summary.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Master Summary Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive summary for easy reference\n",
    "summary_doc = [\n",
    "    \"# COMPREHENSIVE RESULTS SUMMARY FOR PAPER\",\n",
    "    \"# Traffic Safety: Analyzing Crash Inequality Across Chicago Neighborhoods\",\n",
    "    \"\",\n",
    "    \"=\" * 80,\n",
    "    \"SECTION 1: DATASET OVERVIEW\",\n",
    "    \"=\" * 80,\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "if 'dataset_stats' in locals():\n",
    "    summary_doc.append(dataset_stats.to_string(index=False))\n",
    "    summary_doc.append(\"\")\n",
    "\n",
    "summary_doc.extend([\n",
    "    \"=\" * 80,\n",
    "    \"SECTION 2: MODEL PERFORMANCE\",\n",
    "    \"=\" * 80,\n",
    "    \"\",\n",
    "])\n",
    "\n",
    "if HAS_MODEL_RESULTS:\n",
    "    summary_doc.append(model_summary.to_string(index=False))\n",
    "    summary_doc.append(\"\")\n",
    "    summary_doc.append(\"Key Insight: Temporal validation provides realistic performance estimates.\")\n",
    "    summary_doc.append(f\"Performance drop from random to temporal: {abs(model_summary.loc[1, 'PR-AUC Drop']):.3f} PR-AUC\")\n",
    "    summary_doc.append(\"\")\n",
    "\n",
    "summary_doc.extend([\n",
    "    \"=\" * 80,\n",
    "    \"SECTION 3: FEATURE IMPORTANCE\",\n",
    "    \"=\" * 80,\n",
    "    \"\",\n",
    "])\n",
    "\n",
    "if 'top_features' in locals():\n",
    "    summary_doc.append(top_features.to_string(index=False))\n",
    "    summary_doc.append(\"\")\n",
    "\n",
    "summary_doc.extend([\n",
    "    \"=\" * 80,\n",
    "    \"SECTION 4: INEQUALITY ANALYSIS\",\n",
    "    \"=\" * 80,\n",
    "    \"\",\n",
    "])\n",
    "\n",
    "if 'inequality_table' in locals():\n",
    "    summary_doc.append(inequality_table.to_string(index=False))\n",
    "    summary_doc.append(\"\")\n",
    "    if 'stat_tests' in locals():\n",
    "        summary_doc.append(\"Statistical Tests:\")\n",
    "        for _, row in stat_tests.iterrows():\n",
    "            summary_doc.append(f\"  {row['test']}: p={row['p_value']:.6f} {'(significant)' if row['significant'] else '(not significant)'}\")\n",
    "        summary_doc.append(\"\")\n",
    "\n",
    "summary_doc.extend([\n",
    "    \"=\" * 80,\n",
    "    \"SECTION 5: SPATIAL ANALYSIS\",\n",
    "    \"=\" * 80,\n",
    "    \"\",\n",
    "])\n",
    "\n",
    "if 'moran_table' in locals():\n",
    "    summary_doc.append(moran_table.to_string(index=False))\n",
    "    summary_doc.append(\"\")\n",
    "    if 'lisa_summary' in locals():\n",
    "        summary_doc.append(\"LISA Cluster Distribution:\")\n",
    "        summary_doc.append(lisa_summary.to_string(index=False))\n",
    "        summary_doc.append(\"\")\n",
    "\n",
    "summary_doc.extend([\n",
    "    \"=\" * 80,\n",
    "    \"SECTION 6: KEY FINDINGS\",\n",
    "    \"=\" * 80,\n",
    "    \"\",\n",
    "    findings_text,\n",
    "    \"\",\n",
    "    \"=\" * 80,\n",
    "    \"FILES GENERATED FOR PAPER\",\n",
    "    \"=\" * 80,\n",
    "    \"\",\n",
    "    \"Tables (CSV + LaTeX):\",\n",
    "    \"  - table1_model_performance.csv/.tex\",\n",
    "    \"  - table2_feature_importance.csv/.tex\",\n",
    "    \"  - table3_inequality_summary.csv/.tex\",\n",
    "    \"  - table4_spatial_autocorrelation.csv/.tex\",\n",
    "    \"  - table5_dataset_statistics.csv/.tex\",\n",
    "    \"\",\n",
    "    \"Figures:\",\n",
    "    \"  - temporal_model_results.png (ROC, PR, confusion matrix, feature importance)\",\n",
    "    \"  - inequality_analysis.png (6-panel inequality visualization)\",\n",
    "    \"  - moran_scatterplots.png (spatial autocorrelation)\",\n",
    "    \"  - lisa_cluster_map.png (spatial clusters)\",\n",
    "    \"  - map_current_hotspots.png\",\n",
    "    \"  - map_predicted_hotspots.png\",\n",
    "    \"  - map_community_choropleth.png\",\n",
    "    \"  - map_comparison_current_vs_predicted.png\",\n",
    "    \"  - map_hotspot_agreement.png\",\n",
    "    \"\",\n",
    "    \"=\" * 80,\n",
    "    \"END OF SUMMARY\",\n",
    "    \"=\" * 80,\n",
    "])\n",
    "\n",
    "summary_text = \"\\n\".join(summary_doc)\n",
    "\n",
    "# Save master summary\n",
    "with open(RESULTS_DIR / 'MASTER_RESULTS_SUMMARY.txt', 'w') as f:\n",
    "    f.write(summary_text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ MASTER RESULTS SUMMARY CREATED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSaved to: {RESULTS_DIR / 'MASTER_RESULTS_SUMMARY.txt'}\")\n",
    "print(\"\\nThis file contains all key results for your paper in one place.\")\n",
    "print(\"Use it as a reference when writing your ACM SIG format report.\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook consolidates all analysis results into publication-ready formats:\n",
    "\n",
    "### Generated Files:\n",
    "\n",
    "**Tables (CSV + LaTeX):**\n",
    "1. `table1_model_performance` - Model comparison (random vs temporal)\n",
    "2. `table2_feature_importance` - Top 10 features\n",
    "3. `table3_inequality_summary` - Crash rates by income quartile\n",
    "4. `table4_spatial_autocorrelation` - Moran's I results\n",
    "5. `table5_dataset_statistics` - Dataset overview\n",
    "\n",
    "**Summary Documents:**\n",
    "- `key_findings_summary.txt` - Bullet points for abstract/conclusion\n",
    "- `MASTER_RESULTS_SUMMARY.txt` - Comprehensive results reference\n",
    "\n",
    "### For Your Paper:\n",
    "\n",
    "1. **Copy LaTeX tables** directly into your ACM SIG template\n",
    "2. **Reference key findings** when writing abstract and conclusion\n",
    "3. **Use master summary** as a quick reference while writing\n",
    "4. **Include figures** from previous notebooks in Results section\n",
    "\n",
    "### Next Steps:\n",
    "1. Run all analysis notebooks (04, 05, 06) to generate complete results\n",
    "2. Run this notebook to consolidate everything\n",
    "3. Start writing your ACM SIG format paper using these materials"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
